# "Simulate and Recover"
This repository contains code for my COGS 106 class final project. This project aims to evaluate whether the EZ Diffusion Model can accurately recover its own parameters by running a simulate-and-recover experiment. The model's consistency was tested by creating simulated datasets with known true parameters and then attempting to estimate them using the model's inverse equations.

## Overview of the EZ Diffusion Model ##
The EZ Diffusion Model is a cognitive model that allows us to compute estimated parameters directly from observed summary statistics. These parameters have of psychological meaning behind them, and the model is used to analyze data collected from decision making experiments.

## Directory Organization ##
- /src: contains main simulation script and parameter recovery script. Also contains a visualization script
- /test: contains unit tests for the scripts in /src
- /data: stores the csv output file from running the simulation script
- /output: stores the plots generated by the visualization script

## Results ##
The performance of the recovery process was evaluated using two key metrics: bias and squared error, both measured across sample sizes N = 10, 40, 4000.

### Bias ###
Referring to the plots in /output, we can see that the mean bias plot shows how the difference between the true and estimated parameters changes as the sample size increases. In an ideal situation, the bias should average close to zero to reflect that our model works. In bias_plot.png, when the sample size N is small (10), bias fluctuates significantly, particularly for alpha and nu. This is expected because of the limited number of trials that leads to more variability in parameter estimates.

As N increaes, the bias trends for all parameters approach zero, though tau (non-decision time) remains relatively stable, indicating that non-deicision time is recovered with lower variance compared to alpha and nu.

Overall, bias decreases as sample size increases, supporting the claim that the EZ diffusion model is unbiased in parameter recovery when given enough data.

### Bias Squared ###
Referring to the plots in /output, we can see that the mean squared error plot shows how the error in parameter estimation decreases as the sample size grows. For N = 10, squared errors are high because of the small sample size. We can also see that drfit rate is the most sensitive parameter to small sample sizes.

As N increases, MSW decreases for all parameters, confirming that more data leads to better parameter recovery. alpha (boundary separation) and tau (non-decision time) show smoother error reduction while nu (drift rate) decreases in a more linear pattern.

## Conclusion ##
These results confirm that the EZ diffusion model is consistent and unbiased in parameter estimation when given sifficient data. The observed trends align with expectations: bias averages close to zero, and squared error decreases with larger sample sizes.

However, the results also highlight that small sample sizes introduce significant variability, particularly for drift rate nu. This underscores the importance of using sufficiently large sample size in empirical studies relying on diffusion models.

Some key takeaways include:
1. Bias approaches zero as N increases, confirming that the EZ diffusion model does not systematically overestimate or underestimate the parameters.
2. Squared error decreases with larger N, demonstrating that more data improves parameter recovery accuracy. The log-log trend aligns with expectations from statistical theory.
3. Small N (10 trials) produces unreliable estimates, particularly for drift rate nu, emphasizing the need for sufficient sample sizes in cognitive modeling studies.
4. Non-decision time tau is recovered with relatively low error across all sample sizes, suggesting it is the most stable parameter to estimate.
5. Drift rate nu shows the highest variability in both bias and squared error, indicating it is more sensitive to noise and requires larger N for accurate recovery.

### Acknowledgements ###
ChatGPT was used to help with structuring code and debugging, as well as writing some unit tests. Professor Vandekerckhove's slides were also referenced, mainly for equations and specific project requirements.